{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how attention works? each token emits two vectors:\n",
    "* query (what info i'm looking for)\n",
    "* key (what info i contain)\n",
    "\n",
    "How to get affinities between those tokens?\n",
    "* dot product between queries and keys\n",
    "\n",
    "For a single token it looks in the following way:\n",
    "* token's query gets dot producted with all the previous tokens' keys\n",
    "\n",
    "In self-attention, we separately compute queries (ð‘ž) and keys (ð‘˜) to determine how much one token should attend to another. The computed attention weights (wei) capture the compatibility between queries and keys. However, the information that is passed along (aggregated) comes from the â€œvalues.â€ By having a separate linear transformation for values, the network can independently control:\n",
    "* Which information is used to decide the attention weights (via queries and keys), and\n",
    "* Which information is aggregated and passed to the next layer (via values).\n",
    "\n",
    "At the end of an attention block, output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
    "\n",
    "In summary, multiplying ð‘¥ by the value linear layer transforms the token representations into a space where the aggregated information (via the attention weights) is most useful. It decouples the content that is aggregated (values) from the mechanism that decides how to aggregate (queries and keys), giving the model the flexibility to learn both processes independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# single self-attention head\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# here each token produces its key and query\n",
    "k = key(x)    # (B, T, head_size) vector [head_size] - the key of the token\n",
    "q = query(x)  # (B, T, head_size) vecotr [head_size] - the query of the token\n",
    "# communication happens now (basically it's a matrix of affinities), in other words all queries get dot producted with all keys\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "# out = wei @ x\n",
    "v = value(x) # (B, T, head_size)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5599, 0.4401, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3220, 0.2016, 0.4764, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1640, 0.0815, 0.2961, 0.4585, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2051, 0.3007, 0.1894, 0.1808, 0.1241, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0600, 0.1273, 0.0291, 0.0169, 0.0552, 0.7114, 0.0000, 0.0000],\n",
       "        [0.1408, 0.1025, 0.1744, 0.2038, 0.1690, 0.0669, 0.1426, 0.0000],\n",
       "        [0.0223, 0.1086, 0.0082, 0.0040, 0.0080, 0.7257, 0.0216, 0.1016]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the last row (aka the eight token) as an example:\n",
    "[0.0223, 0.1086, 0.0082, 0.0040, 0.0080, 0.7257, 0.0216, 0.1016]\n",
    "\n",
    "The eight token (0.1016) finds the sixth token the most interesting (0.7257)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
