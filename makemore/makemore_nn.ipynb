{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " '.': 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters = sorted(list(set(''.join(words))))\n",
    "str_to_inx = {str:inx+1 for inx, str in enumerate(letters)}\n",
    "str_to_inx['.'] = 0\n",
    "inx_to_str = {inx:str for str, inx in str_to_inx.items()}\n",
    "str_to_inx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  5, 13, 13,  1]), tensor([ 5, 13, 13,  1,  0]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the training set of bigrams (x,y)\n",
    "xs, ys = [], []\n",
    "for w in words[:1]:\n",
    "    chars = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        xi = str_to_inx[ch1]\n",
    "        yi = str_to_inx[ch2]\n",
    "        xs.append(xi)\n",
    "        ys.append(yi)\n",
    "\n",
    "xs, ys = torch.tensor(xs), torch.tensor(ys)\n",
    "xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encoding integers\n",
    "import torch.nn.functional as F\n",
    "x_enc = F.one_hot(xs, num_classes=27).float()\n",
    "y_enc = F.one_hot(ys, num_classes=27).float()\n",
    "x_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd1f3703f40>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _draw_all_if_interactive at 0x7fd1f3afc790> (for post_execute):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/pyplot.py:119\u001b[0m, in \u001b[0;36m_draw_all_if_interactive\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_draw_all_if_interactive\u001b[39m():\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m matplotlib\u001b[39m.\u001b[39mis_interactive():\n\u001b[0;32m--> 119\u001b[0m         draw_all()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/_pylab_helpers.py:132\u001b[0m, in \u001b[0;36mGcf.draw_all\u001b[0;34m(cls, force)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mfor\u001b[39;00m manager \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mget_all_fig_managers():\n\u001b[1;32m    131\u001b[0m     \u001b[39mif\u001b[39;00m force \u001b[39mor\u001b[39;00m manager\u001b[39m.\u001b[39mcanvas\u001b[39m.\u001b[39mfigure\u001b[39m.\u001b[39mstale:\n\u001b[0;32m--> 132\u001b[0m         manager\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mdraw_idle()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/backend_bases.py:2054\u001b[0m, in \u001b[0;36mFigureCanvasBase.draw_idle\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2052\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_idle_drawing:\n\u001b[1;32m   2053\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_idle_draw_cntx():\n\u001b[0;32m-> 2054\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdraw(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:405\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[39mwith\u001b[39;00m RendererAgg\u001b[39m.\u001b[39mlock, \\\n\u001b[1;32m    403\u001b[0m      (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoolbar\u001b[39m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoolbar\n\u001b[1;32m    404\u001b[0m       \u001b[39melse\u001b[39;00m nullcontext()):\n\u001b[0;32m--> 405\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49mdraw(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrenderer)\n\u001b[1;32m    406\u001b[0m     \u001b[39m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[1;32m    407\u001b[0m     \u001b[39m# don't forget to call the superclass.\u001b[39;00m\n\u001b[1;32m    408\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mdraw()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/artist.py:74\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m@wraps\u001b[39m(draw)\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdraw_wrapper\u001b[39m(artist, renderer, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 74\u001b[0m     result \u001b[39m=\u001b[39m draw(artist, renderer, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m renderer\u001b[39m.\u001b[39m_rasterizing:\n\u001b[1;32m     76\u001b[0m         renderer\u001b[39m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/figure.py:3082\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3079\u001b[0m         \u001b[39m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   3081\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 3082\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[1;32m   3083\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[1;32m   3085\u001b[0m \u001b[39mfor\u001b[39;00m sfig \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubfigs:\n\u001b[1;32m   3086\u001b[0m     sfig\u001b[39m.\u001b[39mdraw(renderer)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[1;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axes/_base.py:3100\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3097\u001b[0m         a\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[1;32m   3098\u001b[0m     renderer\u001b[39m.\u001b[39mstop_rasterizing()\n\u001b[0;32m-> 3100\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[1;32m   3101\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[1;32m   3103\u001b[0m renderer\u001b[39m.\u001b[39mclose_group(\u001b[39m'\u001b[39m\u001b[39maxes\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   3104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstale \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[1;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axis.py:1314\u001b[0m, in \u001b[0;36mAxis.draw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1308\u001b[0m     tick\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[1;32m   1310\u001b[0m \u001b[39m# Scale up the axis label box to also find the neighbors, not just the\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[39m# tick labels that actually overlap.  We need a *copy* of the axis\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[39m# label box because we don't want to scale the actual bbox.\u001b[39;00m\n\u001b[0;32m-> 1314\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_label_position(renderer)\n\u001b[1;32m   1316\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[1;32m   1318\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_offset_text_position(tlb1, tlb2)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axis.py:2506\u001b[0m, in \u001b[0;36mYAxis._update_label_position\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2502\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   2504\u001b[0m \u001b[39m# get bounding boxes for this axis and any siblings\u001b[39;00m\n\u001b[1;32m   2505\u001b[0m \u001b[39m# that have been set by `fig.align_ylabels()`\u001b[39;00m\n\u001b[0;32m-> 2506\u001b[0m bboxes, bboxes2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_tick_boxes_siblings(renderer\u001b[39m=\u001b[39;49mrenderer)\n\u001b[1;32m   2507\u001b[0m x, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel\u001b[39m.\u001b[39mget_position()\n\u001b[1;32m   2508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_position \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axis.py:2055\u001b[0m, in \u001b[0;36mAxis._get_tick_boxes_siblings\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2053\u001b[0m \u001b[39mfor\u001b[39;00m ax \u001b[39min\u001b[39;00m grouper\u001b[39m.\u001b[39mget_siblings(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes):\n\u001b[1;32m   2054\u001b[0m     axis \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(ax, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39maxis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2055\u001b[0m     ticks_to_draw \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49m_update_ticks()\n\u001b[1;32m   2056\u001b[0m     tlb, tlb2 \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39m_get_ticklabel_bboxes(ticks_to_draw, renderer)\n\u001b[1;32m   2057\u001b[0m     bboxes\u001b[39m.\u001b[39mextend(tlb)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axis.py:1195\u001b[0m, in \u001b[0;36mAxis._update_ticks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmajor\u001b[39m.\u001b[39mformatter\u001b[39m.\u001b[39mset_locs(major_locs)\n\u001b[1;32m   1194\u001b[0m \u001b[39mfor\u001b[39;00m tick, loc, label \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(major_ticks, major_locs, major_labels):\n\u001b[0;32m-> 1195\u001b[0m     tick\u001b[39m.\u001b[39;49mupdate_position(loc)\n\u001b[1;32m   1196\u001b[0m     tick\u001b[39m.\u001b[39mset_label1(label)\n\u001b[1;32m   1197\u001b[0m     tick\u001b[39m.\u001b[39mset_label2(label)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axis.py:527\u001b[0m, in \u001b[0;36mYTick.update_position\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgridline\u001b[39m.\u001b[39mset_ydata((loc,))\n\u001b[1;32m    526\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel1\u001b[39m.\u001b[39mset_y(loc)\n\u001b[0;32m--> 527\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel2\u001b[39m.\u001b[39;49mset_y(loc)\n\u001b[1;32m    528\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loc \u001b[39m=\u001b[39m loc\n\u001b[1;32m    529\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstale \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/text.py:1168\u001b[0m, in \u001b[0;36mText.set_y\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_x \u001b[39m=\u001b[39m x\n\u001b[1;32m   1166\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstale \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m-> 1168\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_y\u001b[39m(\u001b[39mself\u001b[39m, y):\n\u001b[1;32m   1169\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \u001b[39m    Set the *y* position of the text.\u001b[39;00m\n\u001b[1;32m   1171\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m    y : float\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1176\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_y \u001b[39m=\u001b[39m y\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1717,  0.5167, -2.1987, -1.0017,  0.8243,  1.6960, -0.1370, -0.7104,\n",
       "          0.8761,  1.3154,  0.2041,  0.3212,  0.6660, -1.1269, -0.0979, -0.1313,\n",
       "          1.0487,  0.1452,  0.5457,  1.5847,  0.6086, -1.0828, -1.8032,  0.1239,\n",
       "          0.5702, -0.2081, -0.9149],\n",
       "        [-0.2868,  1.4737,  0.4996, -1.2936,  0.1305,  0.5001, -2.8895, -0.3441,\n",
       "          2.1952, -0.2213,  0.0064, -0.6124,  0.4116,  0.4819, -0.6982, -0.3994,\n",
       "         -0.1958, -0.0031,  1.0546, -0.8464,  2.3087, -0.3053,  0.0936,  0.6797,\n",
       "          0.9844, -0.0038, -0.4368],\n",
       "        [-0.7432,  1.4771,  0.4376,  2.3264,  1.7528,  0.1866, -0.7116,  1.2456,\n",
       "          0.8936, -0.2394,  0.3375,  0.2217, -1.2613,  0.0611, -0.5338,  0.1063,\n",
       "         -2.1232,  0.1422,  1.5488, -2.2513,  0.7437,  0.0851,  0.4295,  0.5724,\n",
       "          0.2855, -0.9446,  0.8359],\n",
       "        [-0.7432,  1.4771,  0.4376,  2.3264,  1.7528,  0.1866, -0.7116,  1.2456,\n",
       "          0.8936, -0.2394,  0.3375,  0.2217, -1.2613,  0.0611, -0.5338,  0.1063,\n",
       "         -2.1232,  0.1422,  1.5488, -2.2513,  0.7437,  0.0851,  0.4295,  0.5724,\n",
       "          0.2855, -0.9446,  0.8359],\n",
       "        [ 0.6627,  1.0033, -0.2555, -0.2483, -1.2525, -1.0132,  1.8938, -0.4897,\n",
       "         -0.7837,  0.3519, -1.2074, -0.0391,  0.0502, -0.5196, -1.2109,  1.0405,\n",
       "          0.4362, -0.3787, -0.9634, -0.3258,  0.4143,  0.9984,  1.7424,  0.6407,\n",
       "          0.2812, -0.1229, -0.7357]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn((27, 27))\n",
    "x_enc @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0075, 0.0405, 0.0027, 0.0089, 0.0551, 0.1316, 0.0211, 0.0119, 0.0580,\n",
       "         0.0900, 0.0296, 0.0333, 0.0470, 0.0078, 0.0219, 0.0212, 0.0689, 0.0279,\n",
       "         0.0417, 0.1178, 0.0444, 0.0082, 0.0040, 0.0273, 0.0427, 0.0196, 0.0097],\n",
       "        [0.0152, 0.0881, 0.0333, 0.0055, 0.0230, 0.0333, 0.0011, 0.0143, 0.1814,\n",
       "         0.0162, 0.0203, 0.0109, 0.0305, 0.0327, 0.0100, 0.0135, 0.0166, 0.0201,\n",
       "         0.0580, 0.0087, 0.2032, 0.0149, 0.0222, 0.0398, 0.0540, 0.0201, 0.0130],\n",
       "        [0.0090, 0.0825, 0.0292, 0.1928, 0.1086, 0.0227, 0.0092, 0.0654, 0.0460,\n",
       "         0.0148, 0.0264, 0.0235, 0.0053, 0.0200, 0.0110, 0.0209, 0.0023, 0.0217,\n",
       "         0.0886, 0.0020, 0.0396, 0.0205, 0.0289, 0.0334, 0.0250, 0.0073, 0.0434],\n",
       "        [0.0090, 0.0825, 0.0292, 0.1928, 0.1086, 0.0227, 0.0092, 0.0654, 0.0460,\n",
       "         0.0148, 0.0264, 0.0235, 0.0053, 0.0200, 0.0110, 0.0209, 0.0023, 0.0217,\n",
       "         0.0886, 0.0020, 0.0396, 0.0205, 0.0289, 0.0334, 0.0250, 0.0073, 0.0434],\n",
       "        [0.0486, 0.0683, 0.0194, 0.0196, 0.0072, 0.0091, 0.1665, 0.0154, 0.0114,\n",
       "         0.0356, 0.0075, 0.0241, 0.0264, 0.0149, 0.0075, 0.0709, 0.0388, 0.0172,\n",
       "         0.0096, 0.0181, 0.0379, 0.0680, 0.1431, 0.0476, 0.0332, 0.0222, 0.0120]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = x_enc @ W # log-counts \n",
    "counts = logits.exp() # equivalent to N from naive\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMMARY ------------------------------>>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neurons' weights. each neuron receives 27 inputs hence 27 x 27\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd1f1690d90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "# btw: the last 2 lines here are together called a 'softmax'\n",
    "plt.imshow(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "bigram example 1: .e (indexes 0,5)\n",
      "input to the neural net: 0\n",
      "output probabilities from the neural net: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\n",
      "label (actual next character): 5\n",
      "probability assigned by the net to the the correct character: 0.01228625513613224\n",
      "log likelihood: -4.399273872375488\n",
      "negative log likelihood: 4.399273872375488\n",
      "--------\n",
      "bigram example 2: em (indexes 5,13)\n",
      "input to the neural net: 5\n",
      "output probabilities from the neural net: tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
      "        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
      "        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.018050700426101685\n",
      "log likelihood: -4.014570713043213\n",
      "negative log likelihood: 4.014570713043213\n",
      "--------\n",
      "bigram example 3: mm (indexes 13,13)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.026691533625125885\n",
      "log likelihood: -3.623408794403076\n",
      "negative log likelihood: 3.623408794403076\n",
      "--------\n",
      "bigram example 4: ma (indexes 13,1)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character): 1\n",
      "probability assigned by the net to the the correct character: 0.07367686182260513\n",
      "log likelihood: -2.6080665588378906\n",
      "negative log likelihood: 2.6080665588378906\n",
      "--------\n",
      "bigram example 5: a. (indexes 1,0)\n",
      "input to the neural net: 1\n",
      "output probabilities from the neural net: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
      "label (actual next character): 0\n",
      "probability assigned by the net to the the correct character: 0.014977526850998402\n",
      "log likelihood: -4.201204299926758\n",
      "negative log likelihood: 4.201204299926758\n",
      "=========\n",
      "average negative log likelihood, i.e. loss = 3.7693049907684326\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "  # i-th bigram:\n",
    "  x = xs[i].item() # input character index\n",
    "  y = ys[i].item() # label character index\n",
    "  print('--------')\n",
    "  print(f'bigram example {i+1}: {inx_to_str[x]}{inx_to_str[y]} (indexes {x},{y})')\n",
    "  print('input to the neural net:', x)\n",
    "  print('output probabilities from the neural net:', probs[i])\n",
    "  print('label (actual next character):', y)\n",
    "  p = probs[i, y]\n",
    "  print('probability assigned by the net to the the correct character:', p.item())\n",
    "  logp = torch.log(p)\n",
    "  print('log likelihood:', logp.item())\n",
    "  nll = -logp\n",
    "  print('negative log likelihood:', nll.item())\n",
    "  nlls[i] = nll\n",
    "\n",
    "print('=========')\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _---------- OPTIMIZATION -------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neurons' weights. each neuron receives 27 inputs hence 27 x 27\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "         0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "         0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459],\n",
      "        [0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
      "         0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
      "         0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472],\n",
      "        [0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "         0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "         0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126],\n",
      "        [0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "         0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "         0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126],\n",
      "        [0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "         0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "         0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.7693, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "print(probs)\n",
    "loss = -probs[torch.arange(0,5), ys].log().mean() # negative log-likelihood\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "W.grad = None # set to zero the gradient\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.data += -0.2 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = str_to_inx[ch1]\n",
    "    ix2 = str_to_inx[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "input_num = xs.nelement()\n",
    "print('number of examples: ', input_num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4545, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n",
      "tensor(2.4544, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "for _ in range(500):\n",
    "    # forward pass\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    loss = -probs[torch.arange(input_num), ys].log().mean() # negative log-likelihood\n",
    "    print(loss)\n",
    "    \n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    W.data += -50 * W.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mor.\n",
      "axx.\n",
      "minaymoryles.\n",
      "kondlaisah.\n",
      "anchshizarie.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  \n",
    "  out = ''\n",
    "  ix = 0\n",
    "  while True:\n",
    "    \n",
    "    # ----------\n",
    "    # BEFORE:\n",
    "    #p = P[ix]\n",
    "    # ----------\n",
    "    # NOW:\n",
    "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # ----------\n",
    "    \n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out += inx_to_str[ix]\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
