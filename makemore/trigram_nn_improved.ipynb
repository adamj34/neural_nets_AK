{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data set\n",
    "xs, ys = [], []\n",
    "for name in words:\n",
    "    name = '..' + name + '.'\n",
    "    for ch1, ch2, ch3 in zip(name, name[1:], name[2:]):\n",
    "        xs.append(ch1+ch2)\n",
    "        ys.append(ch3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'..': 0, '.a': 1, '.b': 2, '.c': 3, '.d': 4, '.e': 5, '.f': 6, '.g': 7, '.h': 8, '.i': 9, '.j': 10, '.k': 11, '.l': 12, '.m': 13, '.n': 14, '.o': 15, '.p': 16, '.q': 17, '.r': 18, '.s': 19, '.t': 20, '.u': 21, '.v': 22, '.w': 23, '.x': 24, '.y': 25, '.z': 26, 'aa': 27, 'ab': 28, 'ac': 29, 'ad': 30, 'ae': 31, 'af': 32, 'ag': 33, 'ah': 34, 'ai': 35, 'aj': 36, 'ak': 37, 'al': 38, 'am': 39, 'an': 40, 'ao': 41, 'ap': 42, 'aq': 43, 'ar': 44, 'as': 45, 'at': 46, 'au': 47, 'av': 48, 'aw': 49, 'ax': 50, 'ay': 51, 'az': 52, 'ba': 53, 'bb': 54, 'bc': 55, 'bd': 56, 'be': 57, 'bh': 58, 'bi': 59, 'bj': 60, 'bl': 61, 'bn': 62, 'bo': 63, 'br': 64, 'bs': 65, 'bt': 66, 'bu': 67, 'by': 68, 'ca': 69, 'cc': 70, 'cd': 71, 'ce': 72, 'cg': 73, 'ch': 74, 'ci': 75, 'cj': 76, 'ck': 77, 'cl': 78, 'co': 79, 'cp': 80, 'cq': 81, 'cr': 82, 'cs': 83, 'ct': 84, 'cu': 85, 'cx': 86, 'cy': 87, 'cz': 88, 'da': 89, 'db': 90, 'dc': 91, 'dd': 92, 'de': 93, 'df': 94, 'dg': 95, 'dh': 96, 'di': 97, 'dj': 98, 'dk': 99, 'dl': 100, 'dm': 101, 'dn': 102, 'do': 103, 'dq': 104, 'dr': 105, 'ds': 106, 'dt': 107, 'du': 108, 'dv': 109, 'dw': 110, 'dy': 111, 'dz': 112, 'ea': 113, 'eb': 114, 'ec': 115, 'ed': 116, 'ee': 117, 'ef': 118, 'eg': 119, 'eh': 120, 'ei': 121, 'ej': 122, 'ek': 123, 'el': 124, 'em': 125, 'en': 126, 'eo': 127, 'ep': 128, 'eq': 129, 'er': 130, 'es': 131, 'et': 132, 'eu': 133, 'ev': 134, 'ew': 135, 'ex': 136, 'ey': 137, 'ez': 138, 'fa': 139, 'fe': 140, 'ff': 141, 'fg': 142, 'fh': 143, 'fi': 144, 'fk': 145, 'fl': 146, 'fn': 147, 'fo': 148, 'fr': 149, 'fs': 150, 'ft': 151, 'fu': 152, 'fw': 153, 'fy': 154, 'fz': 155, 'ga': 156, 'gb': 157, 'gd': 158, 'ge': 159, 'gf': 160, 'gg': 161, 'gh': 162, 'gi': 163, 'gj': 164, 'gl': 165, 'gm': 166, 'gn': 167, 'go': 168, 'gr': 169, 'gs': 170, 'gt': 171, 'gu': 172, 'gv': 173, 'gw': 174, 'gy': 175, 'gz': 176, 'ha': 177, 'hb': 178, 'hc': 179, 'hd': 180, 'he': 181, 'hf': 182, 'hg': 183, 'hh': 184, 'hi': 185, 'hj': 186, 'hk': 187, 'hl': 188, 'hm': 189, 'hn': 190, 'ho': 191, 'hp': 192, 'hq': 193, 'hr': 194, 'hs': 195, 'ht': 196, 'hu': 197, 'hv': 198, 'hw': 199, 'hy': 200, 'hz': 201, 'ia': 202, 'ib': 203, 'ic': 204, 'id': 205, 'ie': 206, 'if': 207, 'ig': 208, 'ih': 209, 'ii': 210, 'ij': 211, 'ik': 212, 'il': 213, 'im': 214, 'in': 215, 'io': 216, 'ip': 217, 'iq': 218, 'ir': 219, 'is': 220, 'it': 221, 'iu': 222, 'iv': 223, 'iw': 224, 'ix': 225, 'iy': 226, 'iz': 227, 'ja': 228, 'jb': 229, 'jc': 230, 'jd': 231, 'je': 232, 'jh': 233, 'ji': 234, 'jj': 235, 'jk': 236, 'jl': 237, 'jm': 238, 'jn': 239, 'jo': 240, 'jp': 241, 'jr': 242, 'js': 243, 'jt': 244, 'ju': 245, 'jv': 246, 'jw': 247, 'jy': 248, 'ka': 249, 'kb': 250, 'kc': 251, 'kd': 252, 'ke': 253, 'kf': 254, 'kh': 255, 'ki': 256, 'kj': 257, 'kk': 258, 'kl': 259, 'km': 260, 'kn': 261, 'ko': 262, 'kr': 263, 'ks': 264, 'kt': 265, 'ku': 266, 'kv': 267, 'kw': 268, 'ky': 269, 'kz': 270, 'la': 271, 'lb': 272, 'lc': 273, 'ld': 274, 'le': 275, 'lf': 276, 'lg': 277, 'lh': 278, 'li': 279, 'lj': 280, 'lk': 281, 'll': 282, 'lm': 283, 'ln': 284, 'lo': 285, 'lp': 286, 'lq': 287, 'lr': 288, 'ls': 289, 'lt': 290, 'lu': 291, 'lv': 292, 'lw': 293, 'ly': 294, 'lz': 295, 'ma': 296, 'mb': 297, 'mc': 298, 'md': 299, 'me': 300, 'mf': 301, 'mh': 302, 'mi': 303, 'mj': 304, 'mk': 305, 'ml': 306, 'mm': 307, 'mn': 308, 'mo': 309, 'mp': 310, 'mr': 311, 'ms': 312, 'mt': 313, 'mu': 314, 'mv': 315, 'mw': 316, 'my': 317, 'mz': 318, 'na': 319, 'nb': 320, 'nc': 321, 'nd': 322, 'ne': 323, 'nf': 324, 'ng': 325, 'nh': 326, 'ni': 327, 'nj': 328, 'nk': 329, 'nl': 330, 'nm': 331, 'nn': 332, 'no': 333, 'np': 334, 'nq': 335, 'nr': 336, 'ns': 337, 'nt': 338, 'nu': 339, 'nv': 340, 'nw': 341, 'nx': 342, 'ny': 343, 'nz': 344, 'oa': 345, 'ob': 346, 'oc': 347, 'od': 348, 'oe': 349, 'of': 350, 'og': 351, 'oh': 352, 'oi': 353, 'oj': 354, 'ok': 355, 'ol': 356, 'om': 357, 'on': 358, 'oo': 359, 'op': 360, 'oq': 361, 'or': 362, 'os': 363, 'ot': 364, 'ou': 365, 'ov': 366, 'ow': 367, 'ox': 368, 'oy': 369, 'oz': 370, 'pa': 371, 'pb': 372, 'pc': 373, 'pe': 374, 'pf': 375, 'ph': 376, 'pi': 377, 'pj': 378, 'pk': 379, 'pl': 380, 'pm': 381, 'pn': 382, 'po': 383, 'pp': 384, 'pr': 385, 'ps': 386, 'pt': 387, 'pu': 388, 'py': 389, 'qa': 390, 'qe': 391, 'qi': 392, 'ql': 393, 'qm': 394, 'qo': 395, 'qr': 396, 'qs': 397, 'qu': 398, 'qw': 399, 'ra': 400, 'rb': 401, 'rc': 402, 'rd': 403, 're': 404, 'rf': 405, 'rg': 406, 'rh': 407, 'ri': 408, 'rj': 409, 'rk': 410, 'rl': 411, 'rm': 412, 'rn': 413, 'ro': 414, 'rp': 415, 'rq': 416, 'rr': 417, 'rs': 418, 'rt': 419, 'ru': 420, 'rv': 421, 'rw': 422, 'rx': 423, 'ry': 424, 'rz': 425, 'sa': 426, 'sb': 427, 'sc': 428, 'sd': 429, 'se': 430, 'sf': 431, 'sg': 432, 'sh': 433, 'si': 434, 'sj': 435, 'sk': 436, 'sl': 437, 'sm': 438, 'sn': 439, 'so': 440, 'sp': 441, 'sq': 442, 'sr': 443, 'ss': 444, 'st': 445, 'su': 446, 'sv': 447, 'sw': 448, 'sy': 449, 'sz': 450, 'ta': 451, 'tb': 452, 'tc': 453, 'te': 454, 'tf': 455, 'tg': 456, 'th': 457, 'ti': 458, 'tj': 459, 'tl': 460, 'tm': 461, 'tn': 462, 'to': 463, 'tr': 464, 'ts': 465, 'tt': 466, 'tu': 467, 'tv': 468, 'tw': 469, 'tx': 470, 'ty': 471, 'tz': 472, 'ua': 473, 'ub': 474, 'uc': 475, 'ud': 476, 'ue': 477, 'uf': 478, 'ug': 479, 'uh': 480, 'ui': 481, 'uj': 482, 'uk': 483, 'ul': 484, 'um': 485, 'un': 486, 'uo': 487, 'up': 488, 'uq': 489, 'ur': 490, 'us': 491, 'ut': 492, 'uu': 493, 'uv': 494, 'uw': 495, 'ux': 496, 'uy': 497, 'uz': 498, 'va': 499, 'vb': 500, 'vd': 501, 've': 502, 'vh': 503, 'vi': 504, 'vk': 505, 'vl': 506, 'vn': 507, 'vo': 508, 'vr': 509, 'vu': 510, 'vv': 511, 'vy': 512, 'wa': 513, 'wb': 514, 'wd': 515, 'we': 516, 'wf': 517, 'wg': 518, 'wh': 519, 'wi': 520, 'wk': 521, 'wl': 522, 'wm': 523, 'wn': 524, 'wo': 525, 'wr': 526, 'ws': 527, 'wt': 528, 'wu': 529, 'ww': 530, 'wy': 531, 'wz': 532, 'xa': 533, 'xb': 534, 'xc': 535, 'xd': 536, 'xe': 537, 'xf': 538, 'xh': 539, 'xi': 540, 'xl': 541, 'xm': 542, 'xn': 543, 'xo': 544, 'xs': 545, 'xt': 546, 'xu': 547, 'xw': 548, 'xx': 549, 'xy': 550, 'xz': 551, 'ya': 552, 'yb': 553, 'yc': 554, 'yd': 555, 'ye': 556, 'yf': 557, 'yg': 558, 'yh': 559, 'yi': 560, 'yj': 561, 'yk': 562, 'yl': 563, 'ym': 564, 'yn': 565, 'yo': 566, 'yp': 567, 'yq': 568, 'yr': 569, 'ys': 570, 'yt': 571, 'yu': 572, 'yv': 573, 'yw': 574, 'yx': 575, 'yy': 576, 'yz': 577, 'za': 578, 'zb': 579, 'zc': 580, 'zd': 581, 'ze': 582, 'zg': 583, 'zh': 584, 'zi': 585, 'zj': 586, 'zk': 587, 'zl': 588, 'zm': 589, 'zn': 590, 'zo': 591, 'zp': 592, 'zr': 593, 'zs': 594, 'zt': 595, 'zu': 596, 'zv': 597, 'zw': 598, 'zx': 599, 'zy': 600, 'zz': 601}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " 'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mappings\n",
    "str_to_inx_pairs = {str:inx for inx, str in enumerate(sorted(set(xs)))}\n",
    "str_to_inx_letter = {str:inx for inx, str in enumerate(sorted(set(ys)))}\n",
    "str_to_inx_pairs['..'] = 0\n",
    "str_to_inx_letter['.'] = 0\n",
    "inx_to_str_pairs = {inx:str for str, inx in str_to_inx_pairs.items()}\n",
    "inx_to_str_letter = {inx:str for str, inx in str_to_inx_letter.items()}\n",
    "print(str_to_inx_pairs)\n",
    "str_to_inx_letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [str_to_inx_pairs[x] for x in xs]\n",
    "ys = [str_to_inx_letter[y] for y in ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = torch.tensor(xs), torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 602])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes=len(inx_to_str_pairs)).float()\n",
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([602, 27])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((len(inx_to_str_pairs), len(inx_to_str_letter)), generator=g, requires_grad=True)\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce MX130\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data set splits\n",
    "train_range = math.ceil(len(xs) * 0.8)\n",
    "dev_range = (len(xs) - train_range) // 2\n",
    "\n",
    "training_set = xs[:train_range]\n",
    "dev_set = xs[train_range:train_range+dev_range]\n",
    "test_set = xs[train_range+dev_range:]\n",
    "\n",
    "y_training_set = ys[:train_range]\n",
    "y_dev_set = ys[train_range:train_range+dev_range]\n",
    "y_test_set = ys[train_range+dev_range:]\n",
    "\n",
    "assert training_set.nelement() + dev_set.nelement() + test_set.nelement() == xs.nelement(), \"Bad split\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182517\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.3981, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the NN\n",
    "input_num = training_set.nelement()\n",
    "xenc = F.one_hot(training_set, num_classes=len(str_to_inx_pairs)).float()\n",
    "\n",
    "print(input_num)\n",
    "for _ in range(50):\n",
    "    # forward pass\n",
    "    logits = torch.matmul(xenc, W)\n",
    "    # counts = logits.exp()\n",
    "    # probs = counts / counts.sum(1, keepdim=True)\n",
    "    # loss = -probs[torch.arange(input_num), y_training_set].log().mean() + 0.01*(W**2).mean()\n",
    "    loss = F.cross_entropy(logits, y_training_set) + 0.01*(W**2).mean()\n",
    "    # print(loss)\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    W.data += -150 * W.grad\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22814, 602]) torch.Size([602, 27]) torch.Size([22814, 27])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.6422, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dev set evaluation\n",
    "input_num = dev_set.nelement()\n",
    "xenc = F.one_hot(dev_set, num_classes=len(str_to_inx_pairs)).float()\n",
    "\n",
    "logits = xenc @ W\n",
    "# -------- EQUIVALENT TO CROSS_ENTROPY --------\n",
    "# counts = logits.exp()\n",
    "# probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "# loss = -probs[torch.arange(input_num), y_dev_set].log().mean()\n",
    "# -------- EQUIVALENT TO CROSS_ENTROPY --------\n",
    "print(xenc.shape, W.shape, logits.shape)\n",
    "loss = F.cross_entropy(logits, y_dev_set) # nll + softmax\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6578, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test set evaluation\n",
    "input_num = test_set.nelement()\n",
    "xenc = F.one_hot(test_set, num_classes=len(str_to_inx_pairs)).float()\n",
    "\n",
    "logits = xenc @ W\n",
    "loss = F.cross_entropy(logits, y_test_set)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1870, grad_fn=<SelectBackward0>)\n",
      "tensor(-0.5672, grad_fn=<SelectBackward0>)\n",
      "tensor(-0.3240, grad_fn=<SelectBackward0>)\n",
      "tensor(1.0157, grad_fn=<SelectBackward0>)\n",
      "tensor(0.2222, grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor(6.7359, grad_fn=<AddBackward0>),\n",
       " tensor(6.4630, grad_fn=<AddBackward0>),\n",
       " tensor(6.4656, grad_fn=<AddBackward0>),\n",
       " tensor(7.3441, grad_fn=<AddBackward0>),\n",
       " tensor(18.0238, grad_fn=<AddBackward0>)]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tune regularization using dev set\n",
    "input_num = dev_set.nelement()\n",
    "xenc = F.one_hot(dev_set, num_classes=len(str_to_inx_pairs)).float()\n",
    "\n",
    "losses = []\n",
    "for reg_term in [0.001, 0.01, 0.1, 1.0, 10]:\n",
    "    W_tune = torch.randn((len(inx_to_str_pairs), len(inx_to_str_letter)), generator=g, requires_grad=True)\n",
    "    print(W_tune[3,3])\n",
    "    for _ in range(50):\n",
    "        # forward pass\n",
    "        logits = torch.matmul(xenc, W_tune)\n",
    "        loss = F.cross_entropy(logits, y_dev_set) + reg_term*(W_tune**2).mean()\n",
    "        # print(loss)\n",
    "\n",
    "        # backward pass\n",
    "        W_tune.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        # update\n",
    "        W_tune.data += -100 * W.grad\n",
    "    losses.append(loss)\n",
    "losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
